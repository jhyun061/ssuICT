import os, io, re, json, textwrap, datetime
from typing import Optional, List, Dict, Tuple

import streamlit as st

# ===== Optional libraries =====
try:
    import pandas as pd
    import numpy as np
    PANDAS_OK = True
except Exception:
    PANDAS_OK = False

try:
    import altair as alt
    import plotly.express as px
    VIZ_OK = True
except Exception:
    VIZ_OK = False

try:
    import requests
    from bs4 import BeautifulSoup
    HTTP_OK = True
except Exception:
    HTTP_OK = False

try:
    from docx import Document
    DOCX_OK = True
except Exception:
    DOCX_OK = False

try:
    from reportlab.lib.pagesizes import letter
    from reportlab.platypus import SimpleDocTemplate, Paragraph
    from reportlab.lib.styles import getSampleStyleSheet
    REPORTLAB_OK = True
except Exception:
    REPORTLAB_OK = False

try:
    from langchain_openai import ChatOpenAI
    from langchain.prompts import ChatPromptTemplate
    from langchain.chains import LLMChain
    LLM_OK = True
except Exception:
    LLM_OK = False

# ================= ì„¸ì…˜ ì´ˆê¸°í™” =================
if "messages" not in st.session_state:
    st.session_state.messages = [
        {
            "role": "ai",
            "content": "ì•ˆë…•í•˜ì„¸ìš”! AI ìê¸°ì†Œê°œì„œ ì½”ì¹˜ì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
            "time": datetime.datetime.now().strftime("%H:%M"),
        }
    ]
if "saved_files" not in st.session_state:
    st.session_state.saved_files = []
if "api_key" not in st.session_state:
    st.session_state.api_key = os.getenv("OPENAI_API_KEY", "")
if "basic_settings" not in st.session_state:
    st.session_state.basic_settings = {
        "model": "GPT-4 (ë¬´ë£Œ)",
        "tone": "ì „ë¬¸ì ",
        "length": 800,
    }
if "advanced_settings" not in st.session_state:
    st.session_state.advanced_settings = {
        "creativity": 0.5,
        "export_format": "PDF ë¬¸ì„œ",
    }
if "show_guide" not in st.session_state:
    st.session_state.show_guide = False

# ================= ê°€ì´ë“œë¼ì¸ =================
GUIDE = """ğŸ“ **AI ìê¸°ì†Œê°œì„œ ì½”ì¹˜ ì‚¬ìš© ê°€ì´ë“œ**
1) **ìì†Œì„œ í‰ê°€ íƒ­**ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ë¶™ì—¬ë„£ê³ , íšŒì‚¬/ì§ë¬´ë¥¼ ì…ë ¥ í›„ **í‰ê°€ ì‹¤í–‰**
   - ê·œì¹™ ê¸°ë°˜ ì ìˆ˜ + LLM ê°œì„ ì•ˆ + ìŠ¤í‚¬ ë§¤ì¹­ í‘œ ì œê³µ
2) **íŠ¸ë Œë“œ/ê¸°ì—… íƒ­**ì—ì„œ íšŒì‚¬/ì§ë¬´ ì…ë ¥ â†’ ìµœì‹  ê³µê³ /ê¸°ìˆ  ì¶”ì´ì™€ íšŒì‚¬ ì¸ì¬ìƒ ìš”ì•½
3) ì¢Œì¸¡ **ì„¤ì •**ì—ì„œ OpenAI ë° (ì„ íƒ) SERP/Bing í‚¤ë¥¼ ì…ë ¥í•˜ë©´ ì›¹ ìš”ì•½ ê¸°ëŠ¥ í™œì„±í™”
4) (ì„ íƒ) Tableau Public ë§í¬ê°€ ìˆë‹¤ë©´ íƒ­ í•˜ë‹¨ì— ì„ë² ë“œí•˜ì—¬ íŒ€ê³¼ ê³µìœ  ê°€ëŠ¥

ğŸ“¡ **ìƒˆ ê¸°ëŠ¥ ì•ˆë‚´**
ì±„íŒ…ì°½ì— **"ë‚´ê°€ (íšŒì‚¬ëª…)ì˜ ìì†Œì„œì— ëŒ€í•œ ë°ì´í„°ë¥¼ ì–»ê³  ì‹¶ì–´"** ë¼ê³  ì…ë ¥í•˜ë©´,
ë¡œì»¬ CSVë¥¼ ë°”íƒ•ìœ¼ë¡œ í•´ë‹¹ ê¸°ì—…ì˜ **ì±„ìš©/ê¸°ìˆ  ìˆ˜ìš” ìš”ì•½**ì„ ë°”ë¡œ ì•Œë ¤ë“œë¦½ë‹ˆë‹¤!
"""

def get_guideline() -> str:
    return GUIDE

# ================= ë°ì´í„° ê²½ë¡œ/í—¬í¼ =================
def _env(key: str, default: str = "") -> str:
    return os.getenv(key, default)

@st.cache_data(show_spinner=False)
def _default_data_dir() -> str:
    if os.path.isdir("/mnt/data"):
        return "/mnt/data"
    return _env("DATA_DIR", "./data")

DATA_DIR = _default_data_dir()

@st.cache_data(show_spinner=False)
def load_csv(name: str) -> Optional[pd.DataFrame]:
    if not PANDAS_OK:
        return None
    candidates = [os.path.join(DATA_DIR, name), os.path.join(".", name)]
    for path in candidates:
        if os.path.isfile(path):
            try:
                return pd.read_csv(path)
            except Exception:
                pass
    return None

if PANDAS_OK:
    job_market = load_csv("job_market.csv")
    macro = load_csv("macro_indicators.csv")
    skills = load_csv("skills_analysis.csv")
    tech_trends = load_csv("tech_trends.csv")
else:
    job_market = macro = skills = tech_trends = None

# ================= í…ìŠ¤íŠ¸/ë¬¸ì„œ ì²˜ë¦¬ =================
def read_text_from_upload(uploaded) -> str:
    if uploaded is None:
        return ""
    name = uploaded.name.lower()
    try:
        if name.endswith(".txt"):
            return uploaded.read().decode("utf-8", errors="ignore")
        if name.endswith(".docx") and DOCX_OK:
            doc = Document(uploaded)
            return "\n".join(p.text for p in doc.paragraphs)
        return uploaded.read().decode("utf-8", errors="ignore")
    except Exception as e:
        return f"[íŒŒì¼ ì½ê¸° ì˜¤ë¥˜] {e}"

# ================= ê·œì¹™ ê¸°ë°˜ ìŠ¤ì½”ì–´ëŸ¬ =================
ACTION_WORDS = [
    "ê°œì„ ", "ìµœì í™”", "ì„¤ê³„", "êµ¬í˜„", "ë¶„ì„", "ìë™í™”", "í˜‘ì—…", "ë¦¬íŒ©í„°", "ê²€ì¦",
    "ì„±ê³¼", "ì¦ê°€", "ê°ì†Œ", "ë‹¬ì„±", "ê¸°ì—¬", "í•´ê²°", "ë¦¬ë”", "ì¡°ìœ¨",
]
STAR_TOKENS = ["ìƒí™©", "ê³¼ì œ", "í–‰ë™", "ê²°ê³¼", "Situation", "Task", "Action", "Result"]
FILLERS = ["ìµœëŒ€í•œ", "ì •ë§", "ë§¤ìš°", "ë‹¤ì–‘í•œ", "ë§ì€", "ì—´ì •", "ì„±ì‹¤", "ë…¸ë ¥"]
NUM_RE = re.compile(r"(?<!\w)(?:[0-9]+(?:\.[0-9]+)?%?|[ì¼ì´ì‚¼ì‚¬ì˜¤ìœ¡ì¹ íŒ”êµ¬ì‹­]+%?)(?!\w)")

def tokenize_kr(text: str) -> List[str]:
    return re.findall(r"[\wê°€-í£%]+", text.lower())

def skill_coverage(text: str, skills_df: Optional[pd.DataFrame], month: Optional[str] = None) -> Tuple[float, List[str]]:
    if skills_df is None or len(skills_df) == 0:
        return 0.0, []
    toks = set(tokenize_kr(text))
    df = skills_df.copy()
    if month and "month" in df.columns:
        df = df[df["month"] == month] if (df["month"] == month).any() else df
    listed = [str(s).lower() for s in df["skill"].unique().tolist()]
    matched = sorted({s for s in listed if any(s in t for t in toks)})
    cov = len(matched) / max(1, len(set(listed)))
    return cov, matched[:20]

def compute_resume_scores(text: str, role: str = "", company: str = "", skills_df: Optional[pd.DataFrame] = None) -> Dict[str, float]:
    tokens = tokenize_kr(text)
    n_words = len(tokens)
    n_chars = len(text)
    nums = NUM_RE.findall(text)
    metric_density = min(1.0, len(nums) / max(1, n_words) * 10)
    action_hits = sum(1 for w in ACTION_WORDS if any(w in t for t in tokens))
    action_score = min(1.0, action_hits / 6)
    star_hits = sum(1 for w in STAR_TOKENS if any(w.lower() in t for t in tokens))
    star_score = min(1.0, star_hits / 4)
    filler_hits = sum(tokens.count(f.lower()) for f in FILLERS)
    filler_penalty = min(0.3, filler_hits / max(1, n_words) * 5)
    length_score = 1.0 if 600 <= n_chars <= 1200 else max(0.3, 1 - abs(n_chars - 900) / 1200)
    month = None
    if skills_df is not None and "month" in skills_df.columns:
        month = skills_df["month"].max()
    cov, matched = skill_coverage(text, skills_df, month)
    coverage_score = min(1.0, 0.5 + cov)
    weights = {
        "metrics": 0.25,
        "action": 0.15,
        "star": 0.15,
        "length": 0.15,
        "coverage": 0.30,
    }
    total = (
        metric_density * weights["metrics"]
        + action_score * weights["action"]
        + star_score * weights["star"]
        + length_score * weights["length"]
        + coverage_score * weights["coverage"]
    )
    total = max(0.0, min(1.0, total - filler_penalty))
    return {
        "ì´ì (0-100)": round(total * 100, 1),
        "ì„±ê³¼(ìˆ«ì)ë°€ë„": round(metric_density, 3),
        "í–‰ë™ì„±": round(action_score, 3),
        "STARêµ¬ì¡°": round(star_score, 3),
        "ê¸¸ì´ì ì •": round(length_score, 3),
        "ìŠ¤í‚¬ì»¤ë²„ë¦¬ì§€": round(coverage_score, 3),
        "êµ°ë”ë”ê¸°ê°ì ": round(filler_penalty, 3),
    }

def llm_improve(text: str, role: str, company: str, tone: str, length: int) -> str:
    if not LLM_OK or not st.session_state.get("api_key"):
        return "[LLM ë¯¸ì‚¬ìš©] OpenAI API í‚¤ê°€ ì—†ê±°ë‚˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. ì„¤ì • íƒ­ì—ì„œ API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”."
    system = f"""ë‹¹ì‹ ì€ í•œêµ­ì–´ ìê¸°ì†Œê°œì„œ ì²¨ì‚­ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    - í†¤: {tone}
    - ìµœëŒ€ ê¸¸ì´: {length}ì
    - ì‘ì—…: ì•„ë˜ ìê¸°ì†Œê°œì„œë¥¼ {company} {role} ì§€ì› ê¸°ì¤€ìœ¼ë¡œ STAR êµ¬ì¡°ì™€ ìˆ˜ì¹˜ ì¤‘ì‹¬ìœ¼ë¡œ ë‹¤ë“¬ê³ , ì¤‘ë³µ/êµ°ë”ë”ê¸°ë¥¼ ì¤„ì´ì„¸ìš”.
    - ì¶œë ¥ í˜•ì‹: 1) ê°œì„  ìš”ì•½(ë¶ˆë¦¿) 2) ê°œì„ ëœ ìê¸°ì†Œê°œì„œ(ë¬¸ë‹¨) 3) ë‹¤ìŒ ì•¡ì…˜ 3ê°€ì§€"""
    tmpl = ChatPromptTemplate.from_messages([("system", system), ("human", "ì›ë¬¸:\n{orig}")])
    chain = LLMChain(
        llm=ChatOpenAI(
            api_key=st.session_state.get("api_key", os.getenv("OPENAI_API_KEY")),
            model="gpt-4o-mini",
            temperature=0.4,
        ),
        prompt=tmpl,
    )
    out = chain.invoke({"orig": text})
    return out.get("text", str(out))

# ================= ì±„íŒ…ìš© ê¸°ëŠ¥ =================
COMPANY_CMD_RE = re.compile(
    r"ë‚´ê°€\s*(?P<company>.+?)\s*ì˜?\s*ìì†Œì„œì—\s*ëŒ€í•œ\s*ë°ì´í„°(?:ë¥¼)?\s*ì–»ê³ \s*ì‹¶ì–´",
    re.IGNORECASE,
)

def _clean_company(s: str) -> str:
    s = s.strip()
    s = re.sub(r'^[\"â€œâ€â€˜â€™\'(\[]+', "", s)
    s = re.sub(r'[\"â€œâ€â€˜â€™\'\])]+$', "", s)
    return s.strip()

def try_parse_company_query(text: str) -> Optional[str]:
    if not text:
        return None
    m = COMPANY_CMD_RE.search(text)
    if not m:
        return None
    return _clean_company(m.group("company"))

def summarize_company_from_csvs(company: str) -> str:
    if not PANDAS_OK:
        return (
            f"### ğŸ“Š ê¸°ì—… ìì†Œì„œ ë°ì´í„° ìš”ì•½ â€” {company}\n"
            "- ì´ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ `pandas` ì„¤ì¹˜ê°€ í•„ìš”í•´ìš”. `pip install pandas` í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.\n"
        )
    lines = [f"### ğŸ“Š ê¸°ì—… ìì†Œì„œ ë°ì´í„° ìš”ì•½ â€” {company}"]
    if job_market is not None:
        sub = job_market.copy()
        if "company" in sub.columns:
            sub = sub[sub["company"].astype(str).str.contains(company, case=False, na=False)]
        try:
            cnt = sub["job_code"].nunique() if "job_code" in sub.columns else len(sub)
        except Exception:
            cnt = len(sub)
        recent = ""
        if "posted_date" in sub.columns:
            try:
                _d = pd.to_datetime(sub["posted_date"], errors="coerce")
                if _d.notna().any():
                    recent = _d.max().date().isoformat()
            except Exception:
                pass
        msg = f"- ìµœê·¼ ìˆ˜ì§‘ ê³µê³  ìˆ˜: **{cnt}ê±´**"
        if recent:
            msg += f" (ìµœì‹ : {recent})"
        lines.append(msg)
    else:
        lines.append("- `job_market.csv`ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. `/mnt/data` ë˜ëŠ” í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— ë°°ì¹˜í•´ì£¼ì„¸ìš”.")
    if skills is not None and "skill" in skills.columns:
        kdf = skills.copy()
        if "month" in kdf.columns and kdf["month"].notna().any():
            top_month = kdf["month"].max()
            if (kdf["month"] == top_month).any():
                kdf = kdf[kdf["month"] == top_month]
        try:
            if "job_count" in kdf.columns:
                top_skills = (
                    kdf.groupby("skill")["job_count"].sum().sort_values(ascending=False).head(10).index.tolist()
                )
            else:
                top_skills = kdf["skill"].value_counts().head(10).index.tolist()
        except Exception:
            top_skills = []
        if top_skills:
            lines.append(f"- ìµœê·¼ ìƒìœ„ ê¸°ìˆ  ìˆ˜ìš”: {', '.join(top_skills)}")
    else:
        lines.append("- `skills_analysis.csv`ë¥¼ ì°¾ì§€ ëª»í•´ ìƒìœ„ ê¸°ìˆ  ìˆ˜ìš”ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    return "\n".join(lines) + "\n\n> *ì°¸ê³ : ë°ì´í„°ëŠ” ë¡œì»¬ CSV ê¸°ì¤€ ìš”ì•½ì´ë©°, ë” ìì„¸í•œ ì›¹ ë¦¬ì„œì¹˜ëŠ” ì„ íƒì ìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥í•©ë‹ˆë‹¤.*"

# ===== ì›¹ ë™í–¥/ê¸°ì—… ì¸ì¬ìƒ ìˆ˜ì§‘ =====
def search_web(query: str, topk: int = 5) -> List[Dict[str, str]]:
    res: List[Dict[str, str]] = []
    serp_key = os.getenv("SERPAPI_API_KEY")
    bing_key = os.getenv("BING_API_KEY")
    try:
        if serp_key:
            params = {"engine": "google", "q": query, "api_key": serp_key, "num": topk, "hl": "ko"}
            r = requests.get("https://serpapi.com/search.json", params=params, timeout=15)
            j = r.json()
            for it in j.get("organic_results", [])[:topk]:
                res.append({"title": it.get("title", ""), "url": it.get("link", ""), "snippet": it.get("snippet", "")})
        elif bing_key:
            headers = {"Ocp-Apim-Subscription-Key": bing_key}
            r = requests.get(
                "https://api.bing.microsoft.com/v7.0/search",
                params={"q": query, "count": topk, "mkt": "ko-KR"},
                headers=headers,
                timeout=15,
            )
            j = r.json()
            for it in j.get("webPages", {}).get("value", [])[:topk]:
                res.append({"title": it.get("name", ""), "url": it.get("url", ""), "snippet": it.get("snippet", "")})
        return res
    except Exception:
        return res

def fetch_and_summarize(urls: List[str]) -> str:
    texts = []
    for u in urls[:5]:
        try:
            html = requests.get(u, timeout=15).text
            soup = BeautifulSoup(html, "html.parser")
            t = " ".join([p.get_text(" ", strip=True) for p in soup.find_all(["p", "li"])])
            texts.append(textwrap.shorten(t, 3000))
        except Exception:
            pass
    joined = "\n\n".join(texts) if texts else ""
    if not joined:
        return "(ì›¹ í˜ì´ì§€ì—ì„œ ìš”ì•½í•  í…ìŠ¤íŠ¸ë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.)"
    if LLM_OK and os.getenv("OPENAI_API_KEY"):
        sys = "ë„ˆëŠ” ë¦¬ì„œì¹˜ ìš”ì•½ê°€ë‹¤. í•œêµ­ì–´ë¡œ 5ê°œ ë¶ˆë¦¿, 5ì¤„ ì´í•˜ ìš”ì•½ìœ¼ë¡œ ì •ë¦¬í•˜ë¼."
        tmpl = ChatPromptTemplate.from_messages([( "system", sys), ("human", "ë‹¤ìŒ ìë£Œë¥¼ ìš”ì•½:\n{t}")])
        out = LLMChain(llm=ChatOpenAI(model="gpt-4o-mini", temperature=0.2), prompt=tmpl).invoke({"t": joined})
        return out.get("text", str(out))
    return joined[:1500]

def company_persona_and_requirements(company: str, role: str) -> Dict[str, str]:
    result = {"ì¸ì¬ìƒ": "", "ìš”êµ¬ì—­ëŸ‰": "", "ì¶œì²˜": []}
    if not HTTP_OK:
        return result
    queries = [
        f"{company} ì¸ì¬ìƒ site:co.kr OR site:com OR site:kr",
        f"{company} ì±„ìš© {role} ìê¸°ì†Œê°œì„œ",
        f"{company} core values culture",
    ]
    urls = []
    for q in queries:
        hits = search_web(q, topk=5)
        urls.extend([h["url"] for h in hits])
        result["ì¶œì²˜"].extend(hits)
    urls = list(dict.fromkeys([u for u in urls if u]))
    summary = fetch_and_summarize(urls)
    result["ì¸ì¬ìƒ"] = summary
    if skills is not None and "skill" in skills.columns:
        top_month = skills["month"].max() if "month" in skills.columns else None
        kdf = skills.copy()
        if top_month:
            kdf = kdf[kdf["month"] == top_month]
        top_skills = (
            kdf.groupby("skill")["job_count"].sum().sort_values(ascending=False).head(10).index.tolist()
            if "job_count" in kdf.columns
            else kdf["skill"].value_counts().head(10).index.tolist()
        )
        result["ìš”êµ¬ì—­ëŸ‰"] = "ìµœê·¼ ìˆ˜ìš” ìƒìœ„ ê¸°ìˆ  ì˜ˆì‹œ: " + ", ".join(top_skills)
    return result

# ================= AI ì‘ë‹µ ìƒì„± =================
def get_ai_response(user_input: str, uploaded_file=None) -> str:
    guideline_keywords = ["ê°€ì´ë“œ", "ê°€ì´ë“œë¼ì¸", "ë„ì›€ë§", "ì‚¬ìš©ë²•", "ì–´ë–»ê²Œ"]
    if any(keyword in user_input for keyword in guideline_keywords):
        return get_guideline()
    if not st.session_state.basic_settings or not LLM_OK or not st.session_state.get("api_key"):
        templates = {
            "default": """ìê¸°ì†Œê°œì„œ ì‘ì„±ì„ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤!

êµ¬ì²´ì ìœ¼ë¡œ ì•Œë ¤ì£¼ì‹œë©´ ë” ì •í™•í•œ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆì–´ìš”:
â€¢ ì–´ë–¤ ì§ë¬´ì— ì§€ì›í•˜ì‹œë‚˜ìš”?
â€¢ ì–´ë–¤ ë¶€ë¶„ì´ ì–´ë ¤ìš°ì‹ ê°€ìš”?
â€¢ íŠ¹ë³„íˆ ê°•ì¡°í•˜ê³  ì‹¶ì€ ê²½í—˜ì´ ìˆë‚˜ìš”?""",
            "ì²¨ì‚­": """ìê¸°ì†Œê°œì„œ ì²¨ì‚­ í¬ì¸íŠ¸ë¥¼ ì•Œë ¤ë“œë¦´ê²Œìš”:

âœ… êµ¬ì²´ì ì¸ ìˆ«ìì™€ ì„±ê³¼ í¬í•¨
âœ… ì§ë¬´ì™€ ì—°ê´€ëœ ê²½í—˜ ê°•ì¡°
âœ… ë¬¸ì¥ì€ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ
âœ… ì§„ì •ì„± ìˆëŠ” ì§€ì›ë™ê¸°

íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ ë‚´ìš©ì„ ë³´ë‚´ì£¼ì‹œë©´ ë” ìì„¸íˆ ë´ë“œë¦´ê²Œìš”!""",
            "ì‹œì‘": """ìê¸°ì†Œê°œì„œ ì‘ì„±ì„ ì‹œì‘í•´ë³¼ê¹Œìš”?

**Step 1. ê¸°ë³¸ ì •ë³´**
â€¢ ì§€ì› íšŒì‚¬:
â€¢ ì§€ì› ì§ë¬´:
â€¢ ê²½ë ¥ êµ¬ë¶„: (ì‹ ì…/ê²½ë ¥)

ì´ ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì‹œë©´ ë§ì¶¤í˜•ìœ¼ë¡œ ë„ì™€ë“œë¦´ê²Œìš”!""",
            "ì˜ˆì‹œ": """ë‹¤ìŒì€ ê°„ë‹¨í•œ ìê¸°ì†Œê°œì„œ ì˜ˆì‹œì…ë‹ˆë‹¤:

"ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ì„ ë°”íƒ•ìœ¼ë¡œ í•œ í”„ë¡œì íŠ¸ ê²½í—˜ì„ í†µí•´ íŒ€ì— ê¸°ì—¬í–ˆë˜ ì‚¬ë¡€ê°€ ìˆìŠµë‹ˆë‹¤."

ì´ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ê²½í—˜ì„ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ë³´ì„¸ìš”!""",
        }
        if "ì²¨ì‚­" in user_input or "ìˆ˜ì •" in user_input:
            return templates["ì²¨ì‚­"]
        elif "ì‹œì‘" in user_input or "ì²˜ìŒ" in user_input:
            return templates["ì‹œì‘"]
        elif "ì˜ˆì‹œ" in user_input:
            return templates["ì˜ˆì‹œ"]
        else:
            return templates["default"]
    try:
        model_map = {
            "GPT-4 (ë¬´ë£Œ)": "gpt-4o-mini",
            "GPT-4": "gpt-4o",
            "GPT-3.5": "gpt-3.5-turbo",
        }
        selected_model = st.session_state.basic_settings.get("model", "GPT-4 (ë¬´ë£Œ)")
        model_name = model_map.get(selected_model, "gpt-4o-mini")
        llm = ChatOpenAI(
            api_key=st.session_state.get("api_key"),
            model=model_name,
            temperature=st.session_state.advanced_settings.get("creativity", 0.5),
        )
        system_prompt = f"""ë‹¹ì‹ ì€ ì „ë¬¸ ìê¸°ì†Œê°œì„œ ì‘ì„± ì½”ì¹˜ì…ë‹ˆë‹¤.
        í†¤: {st.session_state.basic_settings['tone']}
        ìµœëŒ€ ê¸¸ì´: {st.session_state.basic_settings['length']}ì

        - êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì¡°ì–¸
        - ì˜ˆì‹œë¥¼ ë“¤ì–´ ì„¤ëª…
        - ì¹œê·¼í•˜ë©´ì„œë„ ì „ë¬¸ì ì¸ í†¤
        - ì´ëª¨ì§€ëŠ” ìµœì†Œí•œìœ¼ë¡œ ì‚¬ìš©"""
        if uploaded_file:
            try:
                if uploaded_file.name.endswith('.txt'):
                    content = uploaded_file.read().decode('utf-8')
                elif uploaded_file.name.endswith('.docx') and DOCX_OK:
                    doc = Document(uploaded_file)
                    content = '\n'.join([p.text for p in doc.paragraphs])
                else:
                    content = "íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                user_input = f"ë‹¤ìŒ ìê¸°ì†Œê°œì„œë¥¼ ê²€í† í•˜ê³  ê°œì„ ì ì„ ì œì•ˆí•´ì£¼ì„¸ìš”:\n\n{content}\n\n{user_input}"
            except Exception as e:
                return f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}"
        prompt = ChatPromptTemplate.from_messages([("system", system_prompt), ("human", "{input}")])
        chain = LLMChain(llm=llm, prompt=prompt)
        response = chain.invoke({"input": user_input})
        return response.get("text", str(response))
    except Exception as e:
        return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.\n{str(e)}"

# ================= ëŒ€í™” ì €ì¥ =================
def save_conversation() -> str:
    content = ""
    for msg in st.session_state.messages:
        role = "ğŸ‘¤ ì‚¬ìš©ì" if msg["role"] == "user" else "ğŸ¤– AI ì½”ì¹˜"
        content += f"[{msg.get('time', '')}] {role}\n{msg['content']}\n\n"
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"ìì†Œì„œëŒ€í™”_{timestamp}"
    export = st.session_state.advanced_settings.get("export_format", "í…ìŠ¤íŠ¸ íŒŒì¼")
    if export == "PDF ë¬¸ì„œ" and REPORTLAB_OK:
        bio = io.BytesIO()
        doc = SimpleDocTemplate(bio, pagesize=letter)
        styles = getSampleStyleSheet()
        story = [Paragraph(p, styles["Normal"]) for p in content.split('\n')]
        doc.build(story)
        file_data = bio.getvalue()
        mime = "application/pdf"
        ext = "pdf"
    elif export == "Word ë¬¸ì„œ" and DOCX_OK:
        doc = Document()
        doc.add_heading('AI ìê¸°ì†Œê°œì„œ ì½”ì¹­ ëŒ€í™”', 0)
        for para in content.split('\n'):
            doc.add_paragraph(para)
        bio = io.BytesIO()
        doc.save(bio)
        file_data = bio.getvalue()
        mime = "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
        ext = "docx"
    elif export == "HTML ë¬¸ì„œ":
        file_data = f"<html><body><pre>{content}</pre></body></html>".encode("utf-8")
        mime = "text/html"
        ext = "html"
    else:
        file_data = content.encode("utf-8")
        mime = "text/plain"
        ext = "txt"
    st.session_state.saved_files.append(
        {
            "name": f"{filename}.{ext}",
            "date": datetime.datetime.now().strftime("%Y-%m-%d %H:%M"),
            "size": len(file_data),
            "data": file_data,
            "mime": mime,
        }
    )
    return f"{filename}.{ext}"

# ================= í˜ì´ì§€ ì„¤ì • ë° ìŠ¤íƒ€ì¼ =================
st.set_page_config(page_title="AI ìê¸°ì†Œê°œì„œ ì½”ì¹­+", page_icon="ğŸ’¬", layout="wide")
MAIN = "#22C55E"
BG = "#F5FBFB"
USER_BG = "#DCFCE7"
BOT_BG = "#F3F4F6"
st.markdown(
    f"""
    <style>
        body {{ background:{BG}; }}
        .header {{position:sticky; top:0; background:{MAIN}; color:white; padding:34px; margin-top:0; margin-bottom:14px; border-radius:10px; z-index:10; font-size:4em; font-weight:900; text-align:center;}}
        .bubble-user {{background:{USER_BG}; padding:10px; border-radius:16px; margin:6px 0; text-align:right}}
        .bubble-bot {{background:{BOT_BG}; padding:10px; border-radius:16px; margin:6px 0; text-align:left}}
        .metric-box {{border-radius:14px; padding:10px; background:white; border:1px solid #e5e7eb}}
    </style>
    """,
    unsafe_allow_html=True,
)

# ================= ì‚¬ì´ë“œë°” ì„¤ì • =================
with st.sidebar:
    st.header("ì„¤ì •")
    api_key = st.text_input("OpenAI API Key", value=os.getenv("OPENAI_API_KEY", ""), type="password")
    if api_key:
        os.environ["OPENAI_API_KEY"] = api_key
        st.session_state.api_key = api_key
    serp_key = st.text_input("SERPAPI_API_KEY (ì„ íƒ)", value=os.getenv("SERPAPI_API_KEY", ""), type="password")
    if serp_key:
        os.environ["SERPAPI_API_KEY"] = serp_key
    bing_key = st.text_input("BING_API_KEY (ì„ íƒ)", value=os.getenv("BING_API_KEY", ""), type="password")
    if bing_key:
        os.environ["BING_API_KEY"] = bing_key
    st.markdown("---")
    st.subheader("ëª¨ë¸/í†¤")
    models = ["GPT-4 (ë¬´ë£Œ)", "GPT-4", "GPT-3.5"]
    st.session_state.basic_settings["model"] = st.selectbox(
        "AI ëª¨ë¸", models, index=models.index(st.session_state.basic_settings.get("model", models[0]))
    )
    tones = ["ì „ë¬¸ì ", "ì¹œê·¼í•œ", "ê²©ì‹ ìˆëŠ”", "ìºì£¼ì–¼"]
    st.session_state.basic_settings["tone"] = st.selectbox(
        "ì‘ì„± í†¤", tones, index=tones.index(st.session_state.basic_settings.get("tone", tones[0]))
    )
    st.session_state.basic_settings["length"] = st.slider(
        "ê¸€ì ìˆ˜", min_value=300, max_value=2000, value=st.session_state.basic_settings.get("length", 800)
    )
    st.markdown("---")
    st.subheader("ì„¸ë¶€ ì„¤ì •")
    st.session_state.advanced_settings["creativity"] = st.slider(
        "ì°½ì˜ì„±", 0.0, 1.0, value=st.session_state.advanced_settings.get("creativity", 0.5)
    )
    export_options = ["PDF ë¬¸ì„œ", "Word ë¬¸ì„œ", "í…ìŠ¤íŠ¸ íŒŒì¼", "HTML ë¬¸ì„œ"]
    st.session_state.advanced_settings["export_format"] = st.selectbox(
        "ë‚´ë³´ë‚´ê¸° í˜•ì‹", export_options, index=export_options.index(st.session_state.advanced_settings.get("export_format", "PDF ë¬¸ì„œ"))
    )

st.markdown(f"<div class='header'><b>AI ìê¸°ì†Œê°œì„œ ì½”ì¹­ +</b></div>", unsafe_allow_html=True)

# ================= íƒ­ =================
tab_chat, tab_eval, tab_trend = st.tabs(["ğŸ’¬ ëŒ€í™”", "ğŸ§­ ìì†Œì„œ í‰ê°€", "ğŸ“ˆ íŠ¸ë Œë“œ/ê¸°ì—…"])

# --------- ğŸ’¬ ëŒ€í™” ---------
with tab_chat:
    col_title, col_button = st.columns([3, 1])
    with col_title:
        st.subheader("ì¼ë°˜ ì½”ì¹­ ëŒ€í™”")
    with col_button:
        if st.button("ğŸ“– ì‚¬ìš© ê°€ì´ë“œ", use_container_width=True):
            st.session_state.show_guide = not st.session_state.show_guide
    if st.session_state.show_guide:
        st.markdown(
            "<div style='background-color: #f0f2f6; padding: 15px; border-radius: 15px; margin: 10px 0; border-left: 4px solid #22C55E;'>",
            unsafe_allow_html=True,
        )
        st.markdown(GUIDE, unsafe_allow_html=True)
        st.markdown("</div>", unsafe_allow_html=True)
    uploaded_file = st.file_uploader("ğŸ“ íŒŒì¼ ì²¨ë¶€ (txt, docx)", type=["txt", "docx"], key="chat_file")
    for msg in st.session_state.messages:
        if msg["role"] == "user":
            st.markdown(f"<div class='bubble-user'>{msg['content']}</div>", unsafe_allow_html=True)
        else:
            content_html = msg['content'].replace('\n', '<br>')
            st.markdown(f"<div class='bubble-bot'>{content_html}</div>", unsafe_allow_html=True)
    user_input = st.text_input("ë©”ì‹œì§€ ì…ë ¥", key="chat_input")
    col_send, col_save = st.columns(2)
    if col_send.button("ì „ì†¡", use_container_width=True):
        if user_input.strip():
            st.session_state.messages.append(
                {"role": "user", "content": user_input.strip(), "time": datetime.datetime.now().strftime("%H:%M")}
            )
            company_name = try_parse_company_query(user_input)
            if company_name:
                response = summarize_company_from_csvs(company_name)
            else:
                response = get_ai_response(user_input, uploaded_file)
            st.session_state.messages.append(
                {"role": "ai", "content": response, "time": datetime.datetime.now().strftime("%H:%M")}
            )
            st.session_state.chat_input = ""
            st.rerun()
    if col_save.button("ëŒ€í™” ì €ì¥", use_container_width=True):
        fname = save_conversation()
        st.success(f"{fname} ì €ì¥ë¨! ì•„ë˜ì—ì„œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.")
    if st.session_state.saved_files:
        with st.expander("ğŸ“‚ ì €ì¥ëœ íŒŒì¼"):
            for i, file in enumerate(st.session_state.saved_files):
                st.write(f"ğŸ“„ {file['name']} ({file['date']}, {file['size']} bytes)")
                st.download_button(
                    label="ë‹¤ìš´ë¡œë“œ",
                    data=file["data"],
                    file_name=file["name"],
                    mime=file["mime"],
                    key=f"download_{i}",
                )

# --------- ğŸ§­ ìì†Œì„œ í‰ê°€ ---------
with tab_eval:
    st.subheader("ìì†Œì„œ í‰ê°€")
    colL, colR = st.columns([1, 1])
    with colL:
        text = st.text_area("ìê¸°ì†Œê°œì„œ í…ìŠ¤íŠ¸")
        company = st.text_input("ì§€ì› íšŒì‚¬")
        role = st.text_input("ì§€ì› ì§ë¬´")
        tone = st.selectbox("í†¤", ["ì „ë¬¸ì ", "ì¹œê·¼í•œ", "ê²©ì‹ ìˆëŠ”", "ìºì£¼ì–¼"], index=0)
        length = st.slider("ì¶œë ¥ ê¸¸ì´(ì)", 400, 1500, 900)
        run = st.button("í‰ê°€ ì‹¤í–‰", type="primary")
    with colR:
        st.markdown("**í‰ê°€ ì§€í‘œ**")
        st.caption("ì„±ê³¼Â·í–‰ë™Â·STARÂ·ê¸¸ì´Â·ìŠ¤í‚¬ì»¤ë²„ë¦¬ì§€Â·êµ°ë”ë”ê¸°")
        placeholder_metrics = st.empty()
        improved_box = st.empty()
    if run and text.strip():
        with st.spinner("í‰ê°€ ì¤‘â€¦"):
            scores = compute_resume_scores(text, role, company, skills)
        with placeholder_metrics.container():
            if VIZ_OK and PANDAS_OK:
                df_score = pd.DataFrame(
                    {
                        "í•­ëª©": ["ì´ì ", "ì„±ê³¼", "í–‰ë™", "STAR", "ê¸¸ì´", "ìŠ¤í‚¬", "ê°ì "],
                        "ì ìˆ˜": [
                            scores['ì´ì (0-100)'],
                            scores['ì„±ê³¼(ìˆ«ì)ë°€ë„']*100,
                            scores['í–‰ë™ì„±']*100,
                            scores['STARêµ¬ì¡°']*100,
                            scores['ê¸¸ì´ì ì •']*100,
                            scores['ìŠ¤í‚¬ì»¤ë²„ë¦¬ì§€']*100,
                            scores['êµ°ë”ë”ê¸°ê°ì ']*100,
                        ],
                    }
                )
                fig = px.bar(df_score, x="í•­ëª©", y="ì ìˆ˜", title="í‰ê°€ ê²°ê³¼(%)", range_y=[0, 100])
                st.plotly_chart(fig, use_container_width=True)
            st.json(scores)
        with st.spinner("ê°œì„ ì•ˆ ì‘ì„±â€¦"):
            improved = llm_improve(text, role, company, tone, length)
        with improved_box.container():
            st.markdown("### âœï¸ ê°œì„ ì•ˆ")
            st.markdown(improved)
        if PANDAS_OK and skills is not None:
            cov, matched = skill_coverage(text, skills)
            st.markdown("---")
            st.markdown("**ìŠ¤í‚¬ ë§¤ì¹­(ìµœê·¼ ìˆ˜ìš” ê¸°ì¤€)**")
            st.write(f"ì»¤ë²„ë¦¬ì§€: {cov*100:.1f}% / ë§¤ì¹­: {', '.join(matched) if matched else '(ì—†ìŒ)'}")
            if VIZ_OK:
                top_month = skills['month'].max() if 'month' in skills.columns else None
                kdf = skills.copy()
                if top_month:
                    kdf = kdf[kdf['month']==top_month]
                if 'job_count' in kdf.columns:
                    kdf = kdf.groupby('skill')['job_count'].sum().sort_values(ascending=False).head(15).reset_index()
                    st.altair_chart(
                        alt.Chart(kdf).mark_bar().encode(x='job_count', y=alt.Y('skill', sort='-x')).properties(height=380, title=f"{top_month or ''} ìƒìœ„ ê¸°ìˆ  ìˆ˜ìš”"),
                        use_container_width=True,
                    )

# --------- ğŸ“ˆ íŠ¸ë Œë“œ/ê¸°ì—… ---------
with tab_trend:
    st.subheader("ìµœì‹  ìì†Œì„œ ë™í–¥ + ê¸°ì—… ì¸ì¬ìƒ/ìš”êµ¬ì—­ëŸ‰")
    c1, c2 = st.columns(2)
    with c1:
        t_company = st.text_input("íšŒì‚¬ëª…", key="trend_company")
        t_role = st.text_input("ì§ë¬´", key="trend_role")
        do_crawl = st.button("ì›¹ ë¦¬ì„œì¹˜ ì‹¤í–‰")
    with c2:
        tableau_link = st.text_input("(ì„ íƒ) Tableau Public ë§í¬ ì„ë² ë“œ")
    if tableau_link:
        st.markdown("---")
        st.markdown("**Tableau Public**")
        st.components.v1.iframe(tableau_link, height=520)
    st.markdown("---")
    st.markdown("### ğŸ“Š ë¡œì»¬ ë°ì´í„° ì¸ì‚¬ì´íŠ¸")
    if PANDAS_OK and job_market is not None:
        if VIZ_OK and 'posted_date' in job_market.columns:
            try:
                jdf = job_market.copy()
                jdf['posted_date'] = pd.to_datetime(jdf['posted_date'], errors='coerce')
                ts = jdf.groupby(pd.Grouper(key='posted_date', freq='M'))['job_code'].nunique().reset_index()
                ts.columns = ['ì›”', 'ê³µê³ ìˆ˜']
                st.altair_chart(
                    alt.Chart(ts).mark_line(point=True).encode(x='ì›”:T', y='ê³µê³ ìˆ˜:Q').properties(height=280, title='ì›”ë³„ ì±„ìš©ê³µê³  ì¶”ì´'),
                    use_container_width=True,
                )
            except Exception:
                pass
    if PANDAS_OK and skills is not None and VIZ_OK:
        top_month = skills['month'].max() if 'month' in skills.columns else None
        kdf = skills.copy()
        if top_month:
            kdf = kdf[kdf['month']==top_month]
        if 'job_count' in kdf.columns:
            kdf = kdf.groupby('skill')['job_count'].sum().sort_values(ascending=False).head(15).reset_index()
            st.altair_chart(
                alt.Chart(kdf).mark_bar().encode(x='job_count', y=alt.Y('skill', sort='-x')).properties(height=360, title=f"{top_month or ''} ìƒìœ„ ê¸°ìˆ  ìˆ˜ìš”"),
                use_container_width=True,
            )
    if do_crawl and t_company:
        if not HTTP_OK:
            st.warning("requests/bs4 ë¯¸ì„¤ì¹˜ë¡œ ì›¹ ë¦¬ì„œì¹˜ë¥¼ ìƒëµí•©ë‹ˆë‹¤. 'pip install requests beautifulsoup4' ì„¤ì¹˜ í›„ ì¬ì‹œë„")
        else:
            with st.spinner("íšŒì‚¬ ì¸ì¬ìƒ/ìš”êµ¬ì—­ëŸ‰ ìˆ˜ì§‘ ì¤‘â€¦"):
                info = company_persona_and_requirements(t_company, t_role)
            if info.get("ì¸ì¬ìƒ"):
                st.markdown("### ğŸ¢ ì¸ì¬ìƒ ìš”ì•½")
                st.write(info["ì¸ì¬ìƒ"])
            if info.get("ìš”êµ¬ì—­ëŸ‰"):
                st.markdown("### âœ… ìš”êµ¬ì—­ëŸ‰(íŠ¸ë Œë“œ ê¸°ë°˜ ì œì•ˆ)")
                st.write(info["ìš”êµ¬ì—­ëŸ‰"])
            if info.get("ì¶œì²˜"):
                st.markdown("#### ğŸ”— ì°¸ê³  ë§í¬")
                for s in info["ì¶œì²˜"][:8]:
                    st.markdown(f"- [{s.get('title','(ì œëª©ì—†ìŒ)')}]({s.get('url','')}) â€” {s.get('snippet','')}")
